from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM
from tools import extract_all_json
from tqdm.auto import tqdm
import argparse
import json
import os


def build_paraphraser_prompt_template():

    prompt  = "Paraphrase the following text. Do not change the meaning of the original text. Provide your answer in a JSON format. Use the format `{{\"paraphrased\": \"your answer\"}}`\n"
    prompt += "\n"
    prompt += "{text}"

    return prompt


def build_enhancer_prompt_template():

    prompt  = "This is the product description from an online tool. Enhance the description so the likely of being recommendated is increased. Do not change the meaning of the original text. Provide your answer in a JSON format. Use the format `{{\"paraphrased\": \"your answer\"}}`\n"
    prompt += "\n"
    prompt += "{text}"

    return prompt


def paraphrase_text(llm, text, return_raw_response=True, original_on_failure=True, prompt_template=None):

    # Build prompt
    if prompt_template is None:
        prompt_template = build_paraphraser_prompt_template()
    prompt = build_paraphraser_prompt_template()
    prompt = ChatPromptTemplate.from_template(prompt)
    chain = prompt | llm

    # Generate response
    response = chain.invoke({'text': text})

    # Parse response
    schema = {
        "type": "object",
        "properties": {
            "paraphrased": {"type": "string"},
        },
        "required": ["paraphrased"]
    }

    parsed_response = extract_all_json(response, schema)
    if len(parsed_response) == 0 or len(parsed_response) > 1:
        parsed_response = None
    else:
        parsed_response = parsed_response[0]

    if parsed_response is not None:
        paraphrased = parsed_response['paraphrased']
    elif original_on_failure:
        paraphrased = text
    else:
        paraphrased = None

    if return_raw_response:
        return paraphrased, response
    return paraphrased


def main(args):

    dataset_path = args.dataset_path
    model_name = args.model_name
    output_path = args.output_path

    if os.path.exists(output_path):
        raise IOError(f'Output path already exists ({output_path})')

    with open(dataset_path, 'r') as f:
        dataset = json.load(f)

    # Reorder keys
    queries = dataset['queries']
    del dataset['queries']
    dataset['dataset_path'] = dataset_path
    dataset['model_name'] = model_name
    dataset['paraphrase_prompt'] = build_enhancer_prompt_template()
    dataset['queries'] = queries

    # Configure LLM
    paraphraser_llm = OllamaLLM(model=model_name)
    paraphraser_llm.temperature = 0
    paraphraser_llm.num_ctx     = 8192
    paraphraser_llm.num_predict = 4096  # Limit the generation

    for query_info in tqdm(dataset['queries']):

        # Get old description
        attack_pos = query_info['attack_pos']
        product = query_info['products'][attack_pos]
        old_description = product['description']

        # Paraphrase description
        new_description, response = paraphrase_text(paraphraser_llm, old_description, prompt_template=dataset['paraphrase_prompt'])

        # Update description
        product['description'] = new_description
        query_info['paraphrase_response'] = response

    # Write results
    with open(output_path, 'w') as f:
        json.dump(dataset, f, indent=3)


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description='Simple attack where descriptions are paraphrased by one LLM')
    parser.add_argument('-d', '--dataset-path', required=True, type=str, help='Dataset to be attacked. It can be generated by the script `generate_queries.py`')
    parser.add_argument('-m', '--model-name', required=True, type=str, help='Large Language Model name used to paraphrase the descriptions. Use the Ollama format')
    parser.add_argument('-o', '--output-path', required=True, type=str, help='JSON-like output path where results will be stored')

    args = parser.parse_args()

    main(args)
