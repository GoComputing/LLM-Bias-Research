from recommender import RecommendationSystem
from searchengine import AmazonSearchEngine
from langchain_ollama.llms import OllamaLLM
import argparse

def main(args):

    amazon_index_path = args.index_dir
    query = args.query
    llm_name = args.model

    # Load search engine
    print("Loading index...")
    search_engine = AmazonSearchEngine()
    search_engine.load_search_engine(amazon_index_path, top_k=5)

    # Initialize recommender system
    recommender_llm = OllamaLLM(model=llm_name)
    recommendation_system = RecommendationSystem(search_engine, recommender_llm)

    # Generate response
    print("Generating response...")
    matches, response = recommendation_system.query(query)

    print(matches)
    print(response)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Script to test the implemented recommendation system')
    parser.add_argument('-i', '--index-dir', type=str, required=True, help='Path to the store index generated by `create_dataset.py`')
    parser.add_argument('-q', '--query', type=str, required=True, help='Query used to test the system')
    parser.add_argument('-m', '--model', type=str, default='llama3.1', help='LLM model name. This name should be available in ollama')

    args = parser.parse_args()

    main(args)
