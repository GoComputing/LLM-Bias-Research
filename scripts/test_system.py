from recommender import RecommendationSystem, base_prompt_template
from searchengine import AmazonSearchEngine
from langchain_ollama.llms import OllamaLLM
from tqdm.auto import tqdm
import argparse
import json
import os

def main(args):

    amazon_index_path = args.index_dir
    queries_path = args.queries_path
    llm_name = args.model
    output_path = args.output_path

    if os.path.exists(output_path):
        raise IOError('Output path already exists')

    if not os.path.exists(os.path.dirname(output_path)):
        raise IOError('Output directory does not exists')

    if os.path.isdir(output_path):
        raise IOError('Output should be a path to a JSON file, not a directory')

    # Load queries
    with open(queries_path, 'r') as f:
        queries = json.load(f)

    # Load search engine
    print("Loading index...")
    search_engine = AmazonSearchEngine()
    search_engine.load_search_engine(amazon_index_path)

    # Initialize recommender system
    recommender_llm = OllamaLLM(model=llm_name)
    recommender_llm.temperature = 0
    recommendation_system = RecommendationSystem(search_engine, recommender_llm, top_k=5)

    # Generate responses
    res = []
    for query in tqdm(queries):
        matches, response, parsed_response = recommendation_system.query(query)
        res.append({
            "matches": [{
                "title": row['TITLE'],
                "description": row['DESCRIPTION']
            } for row_id, row in matches.iterrows()],
            "response": response,
            "parsed_response": parsed_response
        })

    res = {
        'prompt_template': base_prompt_template(),
        'llm_model_name': llm_name,
        'embedding_model_name': search_engine.embedding_model_name,
        'temperature': f'{recommender_llm.temperature:.5f}',
        'results': res
    }

    # Store results
    with open(output_path, 'w') as f:
        json.dump(res, f, indent=3)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Script to test the implemented recommendation system')
    parser.add_argument('-i', '--index-dir', type=str, required=True, help='Path to the store index generated by `create_dataset.py`')
    parser.add_argument('-q', '--queries-path', type=str, required=True, help='A path to a JSON-like file which holds a list of queries')
    parser.add_argument('-m', '--model', type=str, default='llama3.1', help='LLM model name. This name should be available in ollama')
    parser.add_argument('-o', '--output-path', type=str, required=True, help='A path to a non-existing JSON file where results will be stored')

    args = parser.parse_args()

    main(args)
